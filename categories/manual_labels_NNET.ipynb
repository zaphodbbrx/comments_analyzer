{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "comments = pd.read_csv('../data/comments_lang.csv')\n",
    "comments_en = comments[comments.lang == 'en']\n",
    "vect = CountVectorizer(ngram_range = (1,1), analyzer = 'word',\n",
    "                       stop_words = 'english',\n",
    "                       max_features = 10000,\n",
    "                       min_df = 2, max_df = 0.95).fit(comments_en.Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = list(vect.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "c = enchant.Dict(\"en_UK\")\n",
    "def check_spelling(text):\n",
    "    if not c.check(text):\n",
    "        suggestions = list(set(c.suggest(text)).intersection(set(pw)))\n",
    "        if len(suggestions)>0:\n",
    "            res = suggestions[0]\n",
    "        else:\n",
    "            res = text\n",
    "    else:\n",
    "        res = text\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stupid'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_spelling('stu pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled1 = pd.read_excel('manual_labels.xlsx')\n",
    "labeled2 = pd.read_excel('manual_labels2.xlsx')\n",
    "labeled3 = pd.read_excel('manual_labels3.xlsx')\n",
    "labeled4 = pd.read_excel('manual_labels4.xlsx')\n",
    "labeled5 = pd.read_excel('manual_labels5.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    621\n",
       "2    172\n",
       "5     89\n",
       "0     82\n",
       "3     25\n",
       "1     11\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled5.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    624\n",
       "2    203\n",
       "5     74\n",
       "0     60\n",
       "3     31\n",
       "1      8\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled1.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "def clean_comment(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    deacc = re.sub(r'\\W',' ', text)\n",
    "    tokens = word_tokenize(deacc)\n",
    "    res = ''\n",
    "    for t in tokens:\n",
    "        res += wnl.lemmatize(t)+' '\n",
    "    return res\n",
    "def get_tokens(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    deacc = re.sub(r'\\W',' ', text)\n",
    "    tokens = word_tokenize(deacc)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "import emoji\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def clean_comment(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    deacc = re.sub(r'\\!',' exclamation_point ', text)\n",
    "    tokens = word_tokenize(deacc)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    processed = []\n",
    "    for (word, tag) in tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag!='':\n",
    "            processed.append(wnl.lemmatize(word,wn_tag))\n",
    "        else:\n",
    "            processed.append(wnl.lemmatize(check_spelling(word)))\n",
    "    res = ' '.join(processed)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = pd.concat([labeled1, labeled2, labeled3, labeled4, labeled5],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled.loc[:,'cleaned'] = labeled.Review.apply(clean_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled['tokens'] = labeled.Review.apply(get_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_long = labeled[labeled.tokens.apply(len)>6]\n",
    "labeled_neg = labeled[labeled.label!=4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments_en['cleaned'] = comments_en.Review.apply(clean_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments_en.to_csv('comments_en_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_en = pd.read_csv('comments_en_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dns/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import keras.backend as K\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "\n",
    "#rds = corpus[rd.tokens.apply(len)>5]\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(comments_en.cleaned.tolist())\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(labeled.cleaned)\n",
    "max_length = labeled.tokens.apply(len).max()\n",
    "feats = encoded_docs\n",
    "labels = to_categorical(labeled.label)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, labels, test_size=0.2)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 150, \n",
    "                    input_length=max_length,\n",
    "                   embeddings_regularizer = regularizers.l2(1e-4)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=100, kernel_size=25, padding='same', activation='sigmoid'))\n",
    "model.add(Conv1D(filters=25, kernel_size=25, padding='same', activation='sigmoid'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "#model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(10, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weight('balanced'\n",
    "                                               ,[0,1,2,3,4,5]\n",
    "                                               ,labeled.label.apply(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "3500/4000 [=========================>....] - ETA: 17s - loss: 1.9433 - categorical_accuracy: 0.0489Epoch 00000: val_loss improved from inf to 1.82321, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 148s - loss: 1.9301 - categorical_accuracy: 0.0505 - val_loss: 1.8232 - val_categorical_accuracy: 0.0790\n",
      "Epoch 2/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 1.7832 - categorical_accuracy: 0.1991Epoch 00001: val_loss improved from 1.82321 to 1.73308, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 137s - loss: 1.7789 - categorical_accuracy: 0.2160 - val_loss: 1.7331 - val_categorical_accuracy: 0.6220\n",
      "Epoch 3/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 1.6887 - categorical_accuracy: 0.4709Epoch 00002: val_loss improved from 1.73308 to 1.60474, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 133s - loss: 1.6853 - categorical_accuracy: 0.4765 - val_loss: 1.6047 - val_categorical_accuracy: 0.6220\n",
      "Epoch 4/50\n",
      "3500/4000 [=========================>....] - ETA: 16s - loss: 1.5371 - categorical_accuracy: 0.5911Epoch 00003: val_loss improved from 1.60474 to 1.37894, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 140s - loss: 1.5275 - categorical_accuracy: 0.5948 - val_loss: 1.3789 - val_categorical_accuracy: 0.6220\n",
      "Epoch 5/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 1.4426 - categorical_accuracy: 0.6069Epoch 00004: val_loss improved from 1.37894 to 1.34768, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 138s - loss: 1.4418 - categorical_accuracy: 0.6050 - val_loss: 1.3477 - val_categorical_accuracy: 0.6220\n",
      "Epoch 6/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 1.3627 - categorical_accuracy: 0.6051Epoch 00005: val_loss improved from 1.34768 to 1.18885, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 133s - loss: 1.3563 - categorical_accuracy: 0.6040 - val_loss: 1.1888 - val_categorical_accuracy: 0.6220\n",
      "Epoch 7/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 1.2984 - categorical_accuracy: 0.5989Epoch 00006: val_loss improved from 1.18885 to 1.12664, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 135s - loss: 1.2899 - categorical_accuracy: 0.6020 - val_loss: 1.1266 - val_categorical_accuracy: 0.6220\n",
      "Epoch 8/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 1.2788 - categorical_accuracy: 0.6009Epoch 00007: val_loss did not improve\n",
      "4000/4000 [==============================] - 132s - loss: 1.2735 - categorical_accuracy: 0.6037 - val_loss: 1.1515 - val_categorical_accuracy: 0.6220\n",
      "Epoch 9/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 1.2319 - categorical_accuracy: 0.6029Epoch 00008: val_loss improved from 1.12664 to 1.09419, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 137s - loss: 1.2238 - categorical_accuracy: 0.6053 - val_loss: 1.0942 - val_categorical_accuracy: 0.6220\n",
      "Epoch 10/50\n",
      "3500/4000 [=========================>....] - ETA: 17s - loss: 1.2044 - categorical_accuracy: 0.6034Epoch 00009: val_loss improved from 1.09419 to 1.04080, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 147s - loss: 1.1978 - categorical_accuracy: 0.6053 - val_loss: 1.0408 - val_categorical_accuracy: 0.6220\n",
      "Epoch 11/50\n",
      "3500/4000 [=========================>....] - ETA: 16s - loss: 1.1448 - categorical_accuracy: 0.6054Epoch 00010: val_loss improved from 1.04080 to 0.98104, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 138s - loss: 1.1371 - categorical_accuracy: 0.6050 - val_loss: 0.9810 - val_categorical_accuracy: 0.6220\n",
      "Epoch 12/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 1.1001 - categorical_accuracy: 0.6049Epoch 00011: val_loss improved from 0.98104 to 0.93111, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 133s - loss: 1.0993 - categorical_accuracy: 0.6030 - val_loss: 0.9311 - val_categorical_accuracy: 0.6220\n",
      "Epoch 13/50\n",
      "3500/4000 [=========================>....] - ETA: 16s - loss: 1.0361 - categorical_accuracy: 0.6054Epoch 00012: val_loss improved from 0.93111 to 0.88681, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 142s - loss: 1.0384 - categorical_accuracy: 0.6037 - val_loss: 0.8868 - val_categorical_accuracy: 0.6220\n",
      "Epoch 14/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.9909 - categorical_accuracy: 0.6091Epoch 00013: val_loss improved from 0.88681 to 0.85498, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 134s - loss: 0.9890 - categorical_accuracy: 0.6063 - val_loss: 0.8550 - val_categorical_accuracy: 0.6220\n",
      "Epoch 15/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.9577 - categorical_accuracy: 0.6091Epoch 00014: val_loss improved from 0.85498 to 0.84085, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 132s - loss: 0.9536 - categorical_accuracy: 0.6090 - val_loss: 0.8408 - val_categorical_accuracy: 0.6220\n",
      "Epoch 16/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.9177 - categorical_accuracy: 0.6066Epoch 00015: val_loss improved from 0.84085 to 0.81312, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 133s - loss: 0.9090 - categorical_accuracy: 0.6095 - val_loss: 0.8131 - val_categorical_accuracy: 0.6220\n",
      "Epoch 17/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.8755 - categorical_accuracy: 0.6160Epoch 00016: val_loss improved from 0.81312 to 0.80374, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 136s - loss: 0.8744 - categorical_accuracy: 0.6130 - val_loss: 0.8037 - val_categorical_accuracy: 0.6220\n",
      "Epoch 18/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.8462 - categorical_accuracy: 0.6189Epoch 00017: val_loss improved from 0.80374 to 0.79811, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 133s - loss: 0.8508 - categorical_accuracy: 0.6170 - val_loss: 0.7981 - val_categorical_accuracy: 0.6220\n",
      "Epoch 19/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.8269 - categorical_accuracy: 0.6220Epoch 00018: val_loss did not improve\n",
      "4000/4000 [==============================] - 133s - loss: 0.8231 - categorical_accuracy: 0.6235 - val_loss: 0.8061 - val_categorical_accuracy: 0.6220\n",
      "Epoch 20/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.8134 - categorical_accuracy: 0.6317Epoch 00019: val_loss improved from 0.79811 to 0.79362, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 136s - loss: 0.8096 - categorical_accuracy: 0.6343 - val_loss: 0.7936 - val_categorical_accuracy: 0.6760\n",
      "Epoch 21/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.7971 - categorical_accuracy: 0.6491Epoch 00020: val_loss improved from 0.79362 to 0.78992, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 132s - loss: 0.7964 - categorical_accuracy: 0.6510 - val_loss: 0.7899 - val_categorical_accuracy: 0.7020\n",
      "Epoch 22/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.7862 - categorical_accuracy: 0.6620Epoch 00021: val_loss improved from 0.78992 to 0.78597, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 133s - loss: 0.7830 - categorical_accuracy: 0.6655 - val_loss: 0.7860 - val_categorical_accuracy: 0.7350\n",
      "Epoch 23/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.7693 - categorical_accuracy: 0.6951Epoch 00022: val_loss did not improve\n",
      "4000/4000 [==============================] - 132s - loss: 0.7685 - categorical_accuracy: 0.6968 - val_loss: 0.7860 - val_categorical_accuracy: 0.7460\n",
      "Epoch 24/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.7405 - categorical_accuracy: 0.7126Epoch 00023: val_loss improved from 0.78597 to 0.78090, saving model to weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 131s - loss: 0.7507 - categorical_accuracy: 0.7105 - val_loss: 0.7809 - val_categorical_accuracy: 0.7480\n",
      "Epoch 25/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.7417 - categorical_accuracy: 0.7371Epoch 00024: val_loss did not improve\n",
      "4000/4000 [==============================] - 133s - loss: 0.7345 - categorical_accuracy: 0.7347 - val_loss: 0.7928 - val_categorical_accuracy: 0.7530\n",
      "Epoch 26/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.8083 - categorical_accuracy: 0.7254Epoch 00025: val_loss did not improve\n",
      "4000/4000 [==============================] - 130s - loss: 0.8004 - categorical_accuracy: 0.7238 - val_loss: 0.8380 - val_categorical_accuracy: 0.7140\n",
      "Epoch 27/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.7576 - categorical_accuracy: 0.7074Epoch 00026: val_loss improved from 0.78090 to 0.76742, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 131s - loss: 0.7571 - categorical_accuracy: 0.7115 - val_loss: 0.7674 - val_categorical_accuracy: 0.7520\n",
      "Epoch 28/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.7197 - categorical_accuracy: 0.7494Epoch 00027: val_loss did not improve\n",
      "4000/4000 [==============================] - 134s - loss: 0.7216 - categorical_accuracy: 0.7500 - val_loss: 0.7707 - val_categorical_accuracy: 0.7540\n",
      "Epoch 29/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.6915 - categorical_accuracy: 0.7680Epoch 00028: val_loss did not improve\n",
      "4000/4000 [==============================] - 130s - loss: 0.6873 - categorical_accuracy: 0.7655 - val_loss: 0.7857 - val_categorical_accuracy: 0.7490\n",
      "Epoch 30/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.6689 - categorical_accuracy: 0.7569Epoch 00029: val_loss improved from 0.76742 to 0.74888, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 133s - loss: 0.6721 - categorical_accuracy: 0.7587 - val_loss: 0.7489 - val_categorical_accuracy: 0.7570\n",
      "Epoch 31/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.6456 - categorical_accuracy: 0.7746Epoch 00030: val_loss improved from 0.74888 to 0.74209, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 133s - loss: 0.6445 - categorical_accuracy: 0.7707 - val_loss: 0.7421 - val_categorical_accuracy: 0.7590\n",
      "Epoch 32/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.6059 - categorical_accuracy: 0.7917Epoch 00031: val_loss improved from 0.74209 to 0.73502, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 133s - loss: 0.6107 - categorical_accuracy: 0.7888 - val_loss: 0.7350 - val_categorical_accuracy: 0.7650\n",
      "Epoch 33/50\n",
      "3500/4000 [=========================>....] - ETA: 17s - loss: 0.6027 - categorical_accuracy: 0.7926Epoch 00032: val_loss improved from 0.73502 to 0.72715, saving model to weights.hdf5\n",
      "4000/4000 [==============================] - 149s - loss: 0.6044 - categorical_accuracy: 0.7915 - val_loss: 0.7271 - val_categorical_accuracy: 0.7650\n",
      "Epoch 34/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.5873 - categorical_accuracy: 0.8051Epoch 00033: val_loss did not improve\n",
      "4000/4000 [==============================] - 140s - loss: 0.5862 - categorical_accuracy: 0.8058 - val_loss: 0.7449 - val_categorical_accuracy: 0.7620\n",
      "Epoch 35/50\n",
      "3500/4000 [=========================>....] - ETA: 16s - loss: 0.5772 - categorical_accuracy: 0.8114Epoch 00034: val_loss did not improve\n",
      "4000/4000 [==============================] - 143s - loss: 0.5801 - categorical_accuracy: 0.8093 - val_loss: 0.7714 - val_categorical_accuracy: 0.7600\n",
      "Epoch 36/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.5694 - categorical_accuracy: 0.8123Epoch 00035: val_loss did not improve\n",
      "4000/4000 [==============================] - 129s - loss: 0.5699 - categorical_accuracy: 0.8120 - val_loss: 0.7772 - val_categorical_accuracy: 0.7610\n",
      "Epoch 37/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.5497 - categorical_accuracy: 0.8203Epoch 00036: val_loss did not improve\n",
      "4000/4000 [==============================] - 132s - loss: 0.5602 - categorical_accuracy: 0.8198 - val_loss: 0.7827 - val_categorical_accuracy: 0.7650\n",
      "Epoch 38/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.5540 - categorical_accuracy: 0.8166Epoch 00037: val_loss did not improve\n",
      "4000/4000 [==============================] - 129s - loss: 0.5509 - categorical_accuracy: 0.8175 - val_loss: 0.7889 - val_categorical_accuracy: 0.7600\n",
      "Epoch 39/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.5495 - categorical_accuracy: 0.8186Epoch 00038: val_loss did not improve\n",
      "4000/4000 [==============================] - 129s - loss: 0.5571 - categorical_accuracy: 0.8160 - val_loss: 0.7996 - val_categorical_accuracy: 0.7570\n",
      "Epoch 40/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.5598 - categorical_accuracy: 0.8177Epoch 00039: val_loss did not improve\n",
      "4000/4000 [==============================] - 129s - loss: 0.5550 - categorical_accuracy: 0.8202 - val_loss: 0.8219 - val_categorical_accuracy: 0.7640\n",
      "Epoch 41/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.5439 - categorical_accuracy: 0.8263Epoch 00040: val_loss did not improve\n",
      "4000/4000 [==============================] - 128s - loss: 0.5449 - categorical_accuracy: 0.8263 - val_loss: 0.8044 - val_categorical_accuracy: 0.7570\n",
      "Epoch 42/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.5396 - categorical_accuracy: 0.8286Epoch 00041: val_loss did not improve\n",
      "4000/4000 [==============================] - 130s - loss: 0.5430 - categorical_accuracy: 0.8250 - val_loss: 0.8162 - val_categorical_accuracy: 0.7580\n",
      "Epoch 43/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.5287 - categorical_accuracy: 0.8303Epoch 00042: val_loss did not improve\n",
      "4000/4000 [==============================] - 131s - loss: 0.5274 - categorical_accuracy: 0.8300 - val_loss: 0.8193 - val_categorical_accuracy: 0.7570\n",
      "Epoch 44/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.5189 - categorical_accuracy: 0.8277Epoch 00043: val_loss did not improve\n",
      "4000/4000 [==============================] - 129s - loss: 0.5221 - categorical_accuracy: 0.8280 - val_loss: 0.8332 - val_categorical_accuracy: 0.7600\n",
      "Epoch 45/50\n",
      "3500/4000 [=========================>....] - ETA: 14s - loss: 0.5286 - categorical_accuracy: 0.8306Epoch 00044: val_loss did not improve\n",
      "4000/4000 [==============================] - 130s - loss: 0.5257 - categorical_accuracy: 0.8315 - val_loss: 0.8641 - val_categorical_accuracy: 0.7630\n",
      "Epoch 46/50\n",
      "3500/4000 [=========================>....] - ETA: 15s - loss: 0.5329 - categorical_accuracy: 0.8291Epoch 00045: val_loss did not improve\n",
      "4000/4000 [==============================] - 143s - loss: 0.5349 - categorical_accuracy: 0.8300 - val_loss: 0.8890 - val_categorical_accuracy: 0.7600\n",
      "Epoch 47/50\n",
      "3500/4000 [=========================>....] - ETA: 19s - loss: 0.5250 - categorical_accuracy: 0.8243Epoch 00046: val_loss did not improve\n",
      "4000/4000 [==============================] - 166s - loss: 0.5266 - categorical_accuracy: 0.8275 - val_loss: 0.8792 - val_categorical_accuracy: 0.7510\n",
      "Epoch 48/50\n",
      "3500/4000 [=========================>....] - ETA: 17s - loss: 0.5184 - categorical_accuracy: 0.8340Epoch 00047: val_loss did not improve\n",
      "4000/4000 [==============================] - 162s - loss: 0.5178 - categorical_accuracy: 0.8332 - val_loss: 0.8707 - val_categorical_accuracy: 0.7610\n",
      "Epoch 49/50\n",
      "3500/4000 [=========================>....] - ETA: 19s - loss: 0.5188 - categorical_accuracy: 0.8326Epoch 00048: val_loss did not improve\n",
      "4000/4000 [==============================] - 164s - loss: 0.5144 - categorical_accuracy: 0.8333 - val_loss: 0.8509 - val_categorical_accuracy: 0.7530\n",
      "Epoch 50/50\n",
      "3500/4000 [=========================>....] - ETA: 18s - loss: 0.5160 - categorical_accuracy: 0.8349Epoch 00049: val_loss did not improve\n",
      "4000/4000 [==============================] - 160s - loss: 0.5188 - categorical_accuracy: 0.8312 - val_loss: 0.8590 - val_categorical_accuracy: 0.7520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5557f8d0>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=500,\n",
    "          validation_data = [X_test,y_test],\n",
    "          callbacks=[checkpointer],class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(y_train,y_test,y_train_pred,y_test_pred):\n",
    "    \n",
    "    class_names = ['unknown',\n",
    "        'Crash',\n",
    "        'Balance problems',\n",
    "        'Synchronization',\n",
    "        'Positive',\n",
    "        'Bug']\n",
    "    \n",
    "    class_names_b = ['neg', 'pos']\n",
    "    print('train scores\\n')\n",
    "    print(classification_report(y_train, y_train_pred, target_names = class_names))\n",
    "    print('test scores\\n')\n",
    "    print(classification_report(y_test, y_test_pred, target_names = class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           Other       0.80      0.80      0.80        10\n",
      "           Crash       0.33      0.20      0.25        10\n",
      "Balance problems       0.00      0.00      0.00        10\n",
      " Synchronization       0.29      0.80      0.42        10\n",
      "        Positive       0.60      0.60      0.60        10\n",
      "             Bug       0.00      0.00      0.00        10\n",
      "\n",
      "     avg / total       0.34      0.40      0.35        60\n",
      "\n",
      "model accuracy 0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dns/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,roc_auc_score,accuracy_score, classification_report\n",
    "def eval_network(input_text, model = model):\n",
    "    cleaned_text = clean_comment(input_text)\n",
    "    class_names = ['Other',\n",
    "        'Crash',\n",
    "        'Balance problems',\n",
    "        'Synchronization',\n",
    "        'Positive',\n",
    "        'Bug']\n",
    "    seq = t.texts_to_sequences([cleaned_text])\n",
    "    padded_sequence = sequence.pad_sequences(seq, maxlen=max_length)\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    #print(class_names[prediction[0]])\n",
    "    return np.argmax(class_weight*prediction[0])\n",
    "\n",
    "def val_score(model):\n",
    "    class_names = ['Other',\n",
    "        'Crash',\n",
    "        'Balance problems',\n",
    "        'Synchronization',\n",
    "        'Positive',\n",
    "        'Bug']    \n",
    "    val_en = pd.read_excel('validation_en.xlsx')\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(0,6):\n",
    "        y_true.append([i]*10)\n",
    "        y_pred.append(val_en.iloc[:,i].apply(eval_network))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    print(classification_report(y_true, y_pred, target_names = class_names))\n",
    "    print('model accuracy %1.4f'%(accuracy_score(y_true, y_pred)))\n",
    "    return y_true,y_pred\n",
    "y_true,y_pred = val_score(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_network('this dum gaem crashes every time i launch it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_network('those new weapons are so dam op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_network('The game glitched and all of my trophies and guns are now lost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_network('Cool!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_network('This game is haard to control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_network('Mucho gusto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
