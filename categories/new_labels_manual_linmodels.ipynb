{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "comments = pd.read_csv('comments_en_cleaned.csv')\n",
    "comments_en = comments[comments.lang == 'en']\n",
    "vect = CountVectorizer(ngram_range = (1,1), analyzer = 'word',\n",
    "                       stop_words = 'english',\n",
    "                       max_features = 500,\n",
    "                       min_df = 2, max_df = 0.95).fit(comments_en.Review)\n",
    "pw = list(vect.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "c = enchant.Dict(\"en_UK\")\n",
    "def check_spelling(text):\n",
    "    if not c.check(text):\n",
    "        suggestions = list(set(c.suggest(text)).intersection(set(pw)))\n",
    "        if len(suggestions)>0:\n",
    "            res = suggestions[0]\n",
    "        else:\n",
    "            res = text\n",
    "    else:\n",
    "        res = text\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "import emoji\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def clean_comment(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    deacc = re.sub(r'\\!',' exclamation_point ', text)\n",
    "    tokens = word_tokenize(deacc)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    processed = []\n",
    "    for (word, tag) in tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag!='':\n",
    "            processed.append(wnl.lemmatize(word,wn_tag))\n",
    "        else:\n",
    "            processed.append(wnl.lemmatize(check_spelling(word)))\n",
    "    res = ' '.join(processed)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled4 = pd.read_excel('temp data/for_labeling 4.xlsx').loc[:,['Review', 'Label']]\n",
    "labeled1 = pd.read_excel('temp data/for_labeling 1.xlsx').loc[:,['Review', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = pd.concat([labeled4,labeled1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_nums = {\n",
    "    'Balance':1,\n",
    "    'Graphics':2,\n",
    "    'Bug':3,\n",
    "    'Advertising':4,\n",
    "    'Monetization':5,\n",
    "    'Other':0\n",
    "}\n",
    "labeled['label_num'] = labeled.Label.map(classes_nums)\n",
    "labeled = labeled.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_major_class(df, frac = 0.5):\n",
    "    major_class = df.Label.value_counts().index[0]\n",
    "    dfmc = df[df.Label==major_class].sample(frac = frac)\n",
    "    df_rest = df[df.Label!=major_class]\n",
    "    return pd.concat([dfmc, df_rest],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled = cut_major_class(labeled, frac = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2105, 3)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled.loc[:,'cleaned'] = labeled.Review.apply(clean_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesome'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_spelling('ausume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range = (1,3), analyzer = 'word',\n",
    "                       stop_words = 'english',\n",
    "                       #max_features = 10000,\n",
    "                       min_df = 2, max_df = 0.95).fit(comments_en.cleaned)\n",
    "vocab = vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(y_train,y_test,y_train_pred,y_test_pred):\n",
    "    \n",
    "    class_names = ['Other',\n",
    "        'Balance',\n",
    "        'Graphics',\n",
    "        'Bug',\n",
    "        'Advertising',\n",
    "        'Monetization']\n",
    "    \n",
    "    class_names_b = ['neg', 'pos']\n",
    "    print('train scores\\n')\n",
    "    print(classification_report(y_train, y_train_pred, target_names = class_names))\n",
    "    print('test scores\\n')\n",
    "    print(classification_report(y_test, y_test_pred, target_names = class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scores\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       0.99      0.89      0.94      1375\n",
      "     Balance       0.62      0.95      0.75        41\n",
      "    Graphics       0.59      1.00      0.74        32\n",
      "         Bug       0.71      0.95      0.81       155\n",
      " Advertising       0.16      1.00      0.27         3\n",
      "Monetization       0.71      0.99      0.83        78\n",
      "\n",
      " avg / total       0.94      0.90      0.91      1684\n",
      "\n",
      "test scores\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       0.94      0.82      0.87       339\n",
      "     Balance       0.38      0.50      0.43        10\n",
      "    Graphics       0.40      0.86      0.55         7\n",
      "         Bug       0.48      0.66      0.55        50\n",
      " Advertising       0.00      0.00      0.00         0\n",
      "Monetization       0.32      0.47      0.38        15\n",
      "\n",
      " avg / total       0.84      0.78      0.80       421\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsm/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score: 0.8010\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "vect = CountVectorizer()\n",
    "model = RidgeClassifier()\n",
    "lin_model = Pipeline([('vectorizer', vect), ('classifier', model)])\n",
    "lin_model.set_params(vectorizer__ngram_range = (1,3),vectorizer__analyzer = 'word', vectorizer__stop_words = 'english',\n",
    "                     vectorizer__max_features = 400,\n",
    "                     vectorizer__min_df = 2, vectorizer__max_df = 0.95,\n",
    "                     vectorizer__vocabulary = vocab,\n",
    "                     \n",
    "                     classifier__class_weight = 'balanced', classifier__alpha = 0.5)\n",
    "feats = labeled.cleaned\n",
    "labels = labeled.label_num\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, labels, test_size=0.2)\n",
    "lin_model = lin_model.fit(X_train, y_train)\n",
    "y_train_pred = lin_model.predict(X_train)\n",
    "y_test_pred = lin_model.predict(X_test)\n",
    "eval_model(y_train,y_test,y_train_pred,y_test_pred)\n",
    "print('cross_val_score: %1.4f'% (np.mean(cross_val_score(lin_model, labeled.cleaned,labeled.label_num))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 135 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done 405 out of 405 | elapsed:   54.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "vect = CountVectorizer()\n",
    "model = RidgeClassifier()\n",
    "lin_model = Pipeline([('vectorizer', vect), ('classifier', model)])\n",
    "lin_model.set_params(vectorizer__analyzer = 'word', vectorizer__stop_words = 'english',\n",
    "                     vectorizer__min_df = 2, vectorizer__max_df = 0.95,\n",
    "                     \n",
    "                     classifier__class_weight = 'balanced')\n",
    "\n",
    "param_grid = {\n",
    "        'vectorizer__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "        'vectorizer__max_features': [200,250,300,350,400],        \n",
    "        'classifier__alpha':[0.1, 0.2, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0]\n",
    "    }\n",
    "\n",
    "gs = GridSearchCV(lin_model, cv=3, n_jobs=-1, param_grid=param_grid, verbose = 1)\n",
    "feats = labeled.cleaned\n",
    "labels = labeled.label_num\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, labels, test_size=0.2)\n",
    "gs = gs.fit(feats, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77054631828978626"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Other       0.73      0.73      0.73        11\n",
      "     Balance       0.45      0.83      0.59         6\n",
      "    Graphics       0.50      1.00      0.67         1\n",
      "         Bug       0.89      0.59      0.71        27\n",
      " Advertising       0.38      0.60      0.46         5\n",
      "\n",
      " avg / total       0.74      0.66      0.67        50\n",
      "\n",
      "model accuracy 0.6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsm/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 5, does not match size of target_names, 6\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,roc_auc_score,accuracy_score\n",
    "def eval_classifier(input_text,model):\n",
    "    cleaned_text = clean_comment(input_text)\n",
    "    feats = vect.transform([cleaned_text])\n",
    "    class_names = ['Other',\n",
    "        'Balance',\n",
    "        'Graphics',\n",
    "        'Bug',\n",
    "        'Advertising',\n",
    "        'Monetization']\n",
    "    prediction = model.predict(feats.toarray())\n",
    "    #print(class_names[prediction[0]])\n",
    "    return prediction[0]\n",
    "def eval_pipeline(input_text, model = gs):\n",
    "    cleaned_text = clean_comment(input_text)\n",
    "    class_names = ['Other',\n",
    "        'Balance',\n",
    "        'Graphics',\n",
    "        'Bug',\n",
    "        'Advertising',\n",
    "        'Monetization']\n",
    "    prediction = model.predict([cleaned_text])\n",
    "    #print(class_names[prediction[0]])\n",
    "    return prediction[0]\n",
    "def val_score(model):\n",
    "    class_names = ['Other',\n",
    "        'Balance',\n",
    "        'Graphics',\n",
    "        'Bug',\n",
    "        'Advertising',\n",
    "        'Monetization']   \n",
    "    val_en = pd.read_excel('temp data/val google play.xlsx')\n",
    "    classes_nums = {\n",
    "        'Balance':1,\n",
    "        'Graphics':2,\n",
    "        'Bug':3,\n",
    "        'Advertising':4,\n",
    "        'Monetization':5,\n",
    "        'Other':0\n",
    "    }    \n",
    "    val_en['label_num'] = val_en.Label.map(classes_nums)\n",
    "    y_true = val_en.label_num\n",
    "    y_pred = val_en.Review.apply(eval_pipeline)\n",
    "    print(classification_report(y_true, y_pred, target_names = class_names))\n",
    "    print('model accuracy %1.4f'%(accuracy_score(y_true, y_pred)))\n",
    "    return y_true,y_pred\n",
    "y_true,y_pred = val_score(lin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pipeline('It\\'s good but it crashes way to much and it\\'s so pay to win I could make a whole day ranting about how unfair it is and maybe your game might become better if you listen what other people want like no armor sever and more this game could be better if you listened to what other people want like less overpowered weapons and less overpriced things')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pipeline('it keeps putting me against 20-25 lvl players when i am just 12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pipeline('graphics are strong in this one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pipeline('the game crashes like every 5 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pipeline('Way to many ads. Thats so annoying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pipeline('weapons prices are just crazy! pg3d r u nuts?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pipeline('allo yoba eto ti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
