{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = pd.read_excel('manual_labels.xlsx')\n",
    "unlabeled = pd.read_excel('manual_unlabeled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    624\n",
       "2    203\n",
       "5     74\n",
       "0     60\n",
       "3     31\n",
       "1      8\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled['label'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>lang</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>1764</td>\n",
       "      <td>5</td>\n",
       "      <td>I just love this game I can't delete it like b...</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24448</th>\n",
       "      <td>24448</td>\n",
       "      <td>5</td>\n",
       "      <td>I think in the tutorial u should make to where...</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50689</th>\n",
       "      <td>50689</td>\n",
       "      <td>4</td>\n",
       "      <td>Needs Better Matchmaking  Im in Iron League, a...</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54637</th>\n",
       "      <td>54637</td>\n",
       "      <td>5</td>\n",
       "      <td>Is a great game</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20221</th>\n",
       "      <td>20221</td>\n",
       "      <td>5</td>\n",
       "      <td>R3ALLY COOL EXPIRIANCE!! have u ever played th...</td>\n",
       "      <td>en</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Rating                                             Review  \\\n",
       "1764         1764       5  I just love this game I can't delete it like b...   \n",
       "24448       24448       5  I think in the tutorial u should make to where...   \n",
       "50689       50689       4  Needs Better Matchmaking  Im in Iron League, a...   \n",
       "54637       54637       5                                    Is a great game   \n",
       "20221       20221       5  R3ALLY COOL EXPIRIANCE!! have u ever played th...   \n",
       "\n",
       "      lang  label  \n",
       "1764    en     -1  \n",
       "24448   en     -1  \n",
       "50689   en     -1  \n",
       "54637   en     -1  \n",
       "20221   en     -1  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(input_text,model = m):\n",
    "    feats = vect.transform([input_text])\n",
    "    class_names = ['unknown',\n",
    "        'Crash',\n",
    "        'Balance problems',\n",
    "        'Synchronization',\n",
    "        'Positive',\n",
    "        'Bug']\n",
    "    prediction = model.predict(feats)\n",
    "    #print(class_names[prediction[0]])\n",
    "    return class_names[prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Balance problems\n",
       "1     Synchronization\n",
       "2                 Bug\n",
       "3    Balance problems\n",
       "4    Balance problems\n",
       "5    Balance problems\n",
       "6     Synchronization\n",
       "7    Balance problems\n",
       "8    Balance problems\n",
       "9    Balance problems\n",
       "Name: Balance, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_en.Balance.apply(eval_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat([labeled[['Review', 'label']], unlabeled[['Review','label']]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    12800\n",
       " 4      624\n",
       " 2      203\n",
       " 5       74\n",
       " 0       60\n",
       " 3       31\n",
       " 1        8\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vect = CountVectorizer(ngram_range = (1,3),\n",
    "                       analyzer = 'word',\n",
    "                       stop_words = 'english',\n",
    "                       min_df = 50, max_df = 0.9).fit(total.Review)\n",
    "feats = vect.transform(total.Review).toarray()\n",
    "labels = total.label.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13800, 284)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelSpreading(alpha=0.2, gamma=0.1, kernel='rbf', max_iter=5, n_jobs=1,\n",
       "        n_neighbors=7, tol=0.001)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.semi_supervised import label_propagation\n",
    "from scipy.sparse import csgraph\n",
    "lp_model = label_propagation.LabelSpreading(kernel = 'rbf',gamma = 0.1,alpha = 0.2, max_iter=5)\n",
    "lp_model.fit(feats, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    13421\n",
       "2      204\n",
       "5       76\n",
       "0       60\n",
       "3       31\n",
       "1        8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(lp_model.transduction_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vect = CountVectorizer(ngram_range = (1,3), analyzer = 'word', stop_words = 'english', min_df = 2, max_df = 0.95).fit(labeled.Review)\n",
    "feats = vect.transform(labeled.Review)\n",
    "labels = labeled.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feats, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(y_train,y_test,y_train_pred,y_test_pred):\n",
    "    class_names = ['unknown',\n",
    "        'Crash',\n",
    "        'Balance problems',\n",
    "        'Synchronization',\n",
    "        'Positive',\n",
    "        'Bug']\n",
    "    print('train scores\\n')\n",
    "    print(classification_report(y_train, y_train_pred, target_names = class_names))\n",
    "    print('test scores\\n')\n",
    "    print(classification_report(y_test, y_test_pred, target_names = class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scores\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         unknown       0.33      0.05      0.08        44\n",
      "           Crash       0.50      0.17      0.25         6\n",
      "Balance problems       0.29      0.12      0.17       173\n",
      " Synchronization       0.00      0.00      0.00        27\n",
      "        Positive       0.60      0.53      0.56       487\n",
      "             Bug       0.07      0.32      0.11        63\n",
      "\n",
      "     avg / total       0.46      0.38      0.40       800\n",
      "\n",
      "test scores\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         unknown       0.33      0.06      0.11        16\n",
      "           Crash       0.00      0.00      0.00         2\n",
      "Balance problems       0.06      0.03      0.04        30\n",
      " Synchronization       0.00      0.00      0.00         4\n",
      "        Positive       0.70      0.58      0.63       137\n",
      "             Bug       0.06      0.36      0.11        11\n",
      "\n",
      "     avg / total       0.52      0.43      0.46       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dns/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "m = OneVsRestClassifier(DummyClassifier()).fit(X_train, y_train)\n",
    "y_train_pred = m.predict(X_train)\n",
    "y_test_pred = m.predict(X_test)\n",
    "eval_model(y_train,y_test,y_train_pred,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scores\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         unknown       1.00      0.02      0.04        44\n",
      "           Crash       1.00      0.17      0.29         6\n",
      "Balance problems       0.93      0.64      0.76       173\n",
      " Synchronization       1.00      0.41      0.58        27\n",
      "        Positive       0.76      1.00      0.87       487\n",
      "             Bug       0.91      0.46      0.61        63\n",
      "\n",
      "     avg / total       0.83      0.80      0.76       800\n",
      "\n",
      "test scores\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         unknown       0.00      0.00      0.00        16\n",
      "           Crash       0.00      0.00      0.00         2\n",
      "Balance problems       0.83      0.50      0.62        30\n",
      " Synchronization       1.00      0.25      0.40         4\n",
      "        Positive       0.77      0.99      0.87       137\n",
      "             Bug       0.60      0.27      0.37        11\n",
      "\n",
      "     avg / total       0.71      0.78      0.72       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dns/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "m = LogisticRegression(C = 0.25).fit(X_train, y_train)\n",
    "y_train_pred = m.predict(X_train)\n",
    "y_test_pred = m.predict(X_test)\n",
    "eval_model(y_train,y_test,y_train_pred,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_en = pd.read_excel('validation_en.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(input_text,model = m):\n",
    "    feats = vect.transform([input_text])\n",
    "    class_names = ['unknown',\n",
    "        'Crash',\n",
    "        'Balance problems',\n",
    "        'Synchronization',\n",
    "        'Positive',\n",
    "        'Bug']\n",
    "    prediction = model.predict(feats)\n",
    "    #print(class_names[prediction[0]])\n",
    "    return class_names[prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Balance problems\n",
       "1    Balance problems\n",
       "2    Balance problems\n",
       "3            Positive\n",
       "4     Synchronization\n",
       "5    Balance problems\n",
       "6    Balance problems\n",
       "7            Positive\n",
       "8    Balance problems\n",
       "9            Positive\n",
       "Name: Syncronization, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_en.Syncronization.apply(eval_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
