{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled1 = pd.read_excel('manual_labels.xlsx')\n",
    "labeled2 = pd.read_excel('manual_labels2.xlsx')\n",
    "labeled3 = pd.read_excel('manual_labels3.xlsx')\n",
    "labeled4 = pd.read_excel('manual_labels4.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "def clean_comment(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    deacc = re.sub(r'\\W',' ', text)\n",
    "    tokens = word_tokenize(deacc)\n",
    "    res = ''\n",
    "    for t in tokens:\n",
    "        res += wnl.lemmatize(t)+' '\n",
    "    return res\n",
    "def get_tokens(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    deacc = re.sub(r'\\W',' ', text)\n",
    "    tokens = word_tokenize(deacc)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def clean_comment(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    #deacc = re.sub(r'\\W',' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    processed = []\n",
    "    for (word, tag) in tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag!='':\n",
    "            processed.append(wnl.lemmatize(word,wn_tag))\n",
    "        else:\n",
    "            processed.append(wnl.lemmatize(word))\n",
    "    return ' '.join(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled = pd.concat([labeled1, labeled2, labeled3, labeled4],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled.loc[:,'cleaned'] = labeled.Review.apply(clean_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled['tokens'] = labeled.Review.apply(get_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_long = labeled[labeled.tokens.apply(len)>6]\n",
    "labeled_neg = labeled[labeled.label!=4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsm/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:357: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/lsm/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "comments = pd.read_csv('../data/comments_lang.csv')\n",
    "comments_en = comments[comments.lang == 'en']\n",
    "comments_en.loc[:,'cleaned'] = comments_en.Review.apply(clean_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled.tokens.apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "#from keras.layers.cudnn_recurrent import CuDNNLSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import load_model\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras.backend as K\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(comments_en.cleaned.tolist())\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(labeled.cleaned.tolist())\n",
    "max_length = labeled.tokens.apply(len).max()\n",
    "feats = encoded_docs\n",
    "labels = to_categorical(labeled.label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, labels, test_size=0.2)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_length)\n",
    "#X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1818 samples, validate on 455 samples\n",
      "Epoch 1/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 1.2654 - categorical_accuracy: 0.4161\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.49670, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 23s 13ms/step - loss: 1.2645 - categorical_accuracy: 0.4164 - val_loss: 1.1641 - val_categorical_accuracy: 0.4967\n",
      "Epoch 2/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 1.1087 - categorical_accuracy: 0.4878\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.49670 to 0.50549, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 1.1080 - categorical_accuracy: 0.4879 - val_loss: 0.9972 - val_categorical_accuracy: 0.5055\n",
      "Epoch 3/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.9663 - categorical_accuracy: 0.5406\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.50549 to 0.59560, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.9653 - categorical_accuracy: 0.5407 - val_loss: 0.8701 - val_categorical_accuracy: 0.5956\n",
      "Epoch 4/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.8428 - categorical_accuracy: 0.5839\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.59560 to 0.61319, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.8450 - categorical_accuracy: 0.5831 - val_loss: 0.7846 - val_categorical_accuracy: 0.6132\n",
      "Epoch 5/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.7647 - categorical_accuracy: 0.6511\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.61319 to 0.64176, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.7643 - categorical_accuracy: 0.6507 - val_loss: 0.7572 - val_categorical_accuracy: 0.6418\n",
      "Epoch 6/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.7186 - categorical_accuracy: 0.6828\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.64176 to 0.64396, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 20s 11ms/step - loss: 0.7184 - categorical_accuracy: 0.6832 - val_loss: 0.7432 - val_categorical_accuracy: 0.6440\n",
      "Epoch 7/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.6766 - categorical_accuracy: 0.7033\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.64396 to 0.66374, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 20s 11ms/step - loss: 0.6782 - categorical_accuracy: 0.7030 - val_loss: 0.7208 - val_categorical_accuracy: 0.6637\n",
      "Epoch 8/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.6234 - categorical_accuracy: 0.7228\n",
      "Epoch 00008: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.6263 - categorical_accuracy: 0.7200 - val_loss: 0.6973 - val_categorical_accuracy: 0.6549\n",
      "Epoch 9/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.6030 - categorical_accuracy: 0.7233\n",
      "Epoch 00009: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.6062 - categorical_accuracy: 0.7211 - val_loss: 0.6825 - val_categorical_accuracy: 0.6615\n",
      "Epoch 10/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.5882 - categorical_accuracy: 0.7300\n",
      "Epoch 00010: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.5857 - categorical_accuracy: 0.7316 - val_loss: 0.6653 - val_categorical_accuracy: 0.6527\n",
      "Epoch 11/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.5456 - categorical_accuracy: 0.7444\n",
      "Epoch 00011: val_categorical_accuracy improved from 0.66374 to 0.67692, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.5439 - categorical_accuracy: 0.7453 - val_loss: 0.6451 - val_categorical_accuracy: 0.6769\n",
      "Epoch 12/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.5057 - categorical_accuracy: 0.7661\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.67692 to 0.69231, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.5091 - categorical_accuracy: 0.7635 - val_loss: 0.6266 - val_categorical_accuracy: 0.6923\n",
      "Epoch 13/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.4812 - categorical_accuracy: 0.7856\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.69231 to 0.70769, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 20s 11ms/step - loss: 0.4838 - categorical_accuracy: 0.7833 - val_loss: 0.6058 - val_categorical_accuracy: 0.7077\n",
      "Epoch 14/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.4782 - categorical_accuracy: 0.7867\n",
      "Epoch 00014: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 20s 11ms/step - loss: 0.4784 - categorical_accuracy: 0.7866 - val_loss: 0.6008 - val_categorical_accuracy: 0.7077\n",
      "Epoch 15/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.4714 - categorical_accuracy: 0.7833\n",
      "Epoch 00015: val_categorical_accuracy improved from 0.70769 to 0.71868, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.4719 - categorical_accuracy: 0.7827 - val_loss: 0.6057 - val_categorical_accuracy: 0.7187\n",
      "Epoch 16/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.4571 - categorical_accuracy: 0.7994\n",
      "Epoch 00016: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.4575 - categorical_accuracy: 0.7992 - val_loss: 0.6022 - val_categorical_accuracy: 0.7143\n",
      "Epoch 17/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.4387 - categorical_accuracy: 0.8167\n",
      "Epoch 00017: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.4380 - categorical_accuracy: 0.8163 - val_loss: 0.5952 - val_categorical_accuracy: 0.7121\n",
      "Epoch 18/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.4343 - categorical_accuracy: 0.8161\n",
      "Epoch 00018: val_categorical_accuracy improved from 0.71868 to 0.71868, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.4332 - categorical_accuracy: 0.8168 - val_loss: 0.5787 - val_categorical_accuracy: 0.7187\n",
      "Epoch 19/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.4241 - categorical_accuracy: 0.8250\n",
      "Epoch 00019: val_categorical_accuracy improved from 0.71868 to 0.72308, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.4243 - categorical_accuracy: 0.8256 - val_loss: 0.5875 - val_categorical_accuracy: 0.7231\n",
      "Epoch 20/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.4142 - categorical_accuracy: 0.8222\n",
      "Epoch 00020: val_categorical_accuracy improved from 0.72308 to 0.72967, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.4158 - categorical_accuracy: 0.8218 - val_loss: 0.5882 - val_categorical_accuracy: 0.7297\n",
      "Epoch 21/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3972 - categorical_accuracy: 0.8372\n",
      "Epoch 00021: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.3965 - categorical_accuracy: 0.8377 - val_loss: 0.5922 - val_categorical_accuracy: 0.7209\n",
      "Epoch 22/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3936 - categorical_accuracy: 0.8378\n",
      "Epoch 00022: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.3941 - categorical_accuracy: 0.8383 - val_loss: 0.5855 - val_categorical_accuracy: 0.7275\n",
      "Epoch 23/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3926 - categorical_accuracy: 0.8367\n",
      "Epoch 00023: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.3924 - categorical_accuracy: 0.8366 - val_loss: 0.5941 - val_categorical_accuracy: 0.7231\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3866 - categorical_accuracy: 0.8411\n",
      "Epoch 00024: val_categorical_accuracy improved from 0.72967 to 0.72967, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.3850 - categorical_accuracy: 0.8421 - val_loss: 0.5784 - val_categorical_accuracy: 0.7297\n",
      "Epoch 25/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3770 - categorical_accuracy: 0.8428\n",
      "Epoch 00025: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.3760 - categorical_accuracy: 0.8432 - val_loss: 0.5836 - val_categorical_accuracy: 0.7209\n",
      "Epoch 26/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3750 - categorical_accuracy: 0.8494\n",
      "Epoch 00026: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.3737 - categorical_accuracy: 0.8498 - val_loss: 0.5909 - val_categorical_accuracy: 0.7253\n",
      "Epoch 27/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3784 - categorical_accuracy: 0.8417\n",
      "Epoch 00027: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.3772 - categorical_accuracy: 0.8427 - val_loss: 0.5910 - val_categorical_accuracy: 0.7253\n",
      "Epoch 28/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3650 - categorical_accuracy: 0.8472\n",
      "Epoch 00028: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.3645 - categorical_accuracy: 0.8476 - val_loss: 0.5836 - val_categorical_accuracy: 0.7275\n",
      "Epoch 29/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3666 - categorical_accuracy: 0.8433\n",
      "Epoch 00029: val_categorical_accuracy improved from 0.72967 to 0.73187, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.3663 - categorical_accuracy: 0.8432 - val_loss: 0.5763 - val_categorical_accuracy: 0.7319\n",
      "Epoch 30/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3536 - categorical_accuracy: 0.8550\n",
      "Epoch 00030: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.3525 - categorical_accuracy: 0.8553 - val_loss: 0.5923 - val_categorical_accuracy: 0.7187\n",
      "Epoch 31/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3564 - categorical_accuracy: 0.8533\n",
      "Epoch 00031: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.3569 - categorical_accuracy: 0.8526 - val_loss: 0.5849 - val_categorical_accuracy: 0.7275\n",
      "Epoch 32/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3519 - categorical_accuracy: 0.8506\n",
      "Epoch 00032: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.3499 - categorical_accuracy: 0.8515 - val_loss: 0.6004 - val_categorical_accuracy: 0.7143\n",
      "Epoch 33/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3427 - categorical_accuracy: 0.8583\n",
      "Epoch 00033: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 23s 13ms/step - loss: 0.3440 - categorical_accuracy: 0.8575 - val_loss: 0.5983 - val_categorical_accuracy: 0.7187\n",
      "Epoch 34/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3385 - categorical_accuracy: 0.8572\n",
      "Epoch 00034: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 25s 14ms/step - loss: 0.3393 - categorical_accuracy: 0.8564 - val_loss: 0.5954 - val_categorical_accuracy: 0.7275\n",
      "Epoch 35/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3429 - categorical_accuracy: 0.8556\n",
      "Epoch 00035: val_categorical_accuracy improved from 0.73187 to 0.73187, saving model to weights.hdf5\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.3413 - categorical_accuracy: 0.8564 - val_loss: 0.5849 - val_categorical_accuracy: 0.7319\n",
      "Epoch 36/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3327 - categorical_accuracy: 0.8589\n",
      "Epoch 00036: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.3341 - categorical_accuracy: 0.8581 - val_loss: 0.5971 - val_categorical_accuracy: 0.7231\n",
      "Epoch 37/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3231 - categorical_accuracy: 0.8672\n",
      "Epoch 00037: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.3243 - categorical_accuracy: 0.8663 - val_loss: 0.6117 - val_categorical_accuracy: 0.7077\n",
      "Epoch 38/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3314 - categorical_accuracy: 0.8600\n",
      "Epoch 00038: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.3316 - categorical_accuracy: 0.8592 - val_loss: 0.5965 - val_categorical_accuracy: 0.7253\n",
      "Epoch 39/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3217 - categorical_accuracy: 0.8622\n",
      "Epoch 00039: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.3229 - categorical_accuracy: 0.8614 - val_loss: 0.5953 - val_categorical_accuracy: 0.7209\n",
      "Epoch 40/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3168 - categorical_accuracy: 0.8633\n",
      "Epoch 00040: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.3182 - categorical_accuracy: 0.8625 - val_loss: 0.5933 - val_categorical_accuracy: 0.7297\n",
      "Epoch 41/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3259 - categorical_accuracy: 0.8606\n",
      "Epoch 00041: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.3255 - categorical_accuracy: 0.8603 - val_loss: 0.5896 - val_categorical_accuracy: 0.7275\n",
      "Epoch 42/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3132 - categorical_accuracy: 0.8650\n",
      "Epoch 00042: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.3129 - categorical_accuracy: 0.8652 - val_loss: 0.5980 - val_categorical_accuracy: 0.7275\n",
      "Epoch 43/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3130 - categorical_accuracy: 0.8694\n",
      "Epoch 00043: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.3130 - categorical_accuracy: 0.8696 - val_loss: 0.6104 - val_categorical_accuracy: 0.7099\n",
      "Epoch 44/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3087 - categorical_accuracy: 0.8728\n",
      "Epoch 00044: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.3123 - categorical_accuracy: 0.8702 - val_loss: 0.5857 - val_categorical_accuracy: 0.7297\n",
      "Epoch 45/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3135 - categorical_accuracy: 0.8661\n",
      "Epoch 00045: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.3112 - categorical_accuracy: 0.8674 - val_loss: 0.6163 - val_categorical_accuracy: 0.7077\n",
      "Epoch 46/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3089 - categorical_accuracy: 0.8694\n",
      "Epoch 00046: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 11ms/step - loss: 0.3097 - categorical_accuracy: 0.8691 - val_loss: 0.5999 - val_categorical_accuracy: 0.7253\n",
      "Epoch 47/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3464 - categorical_accuracy: 0.8478\n",
      "Epoch 00047: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.3465 - categorical_accuracy: 0.8476 - val_loss: 0.6187 - val_categorical_accuracy: 0.7055\n",
      "Epoch 48/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3230 - categorical_accuracy: 0.8611\n",
      "Epoch 00048: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 23s 12ms/step - loss: 0.3226 - categorical_accuracy: 0.8619 - val_loss: 0.6119 - val_categorical_accuracy: 0.7143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3086 - categorical_accuracy: 0.8733\n",
      "Epoch 00049: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 22s 12ms/step - loss: 0.3097 - categorical_accuracy: 0.8724 - val_loss: 0.6041 - val_categorical_accuracy: 0.7231\n",
      "Epoch 50/50\n",
      "1800/1818 [============================>.] - ETA: 0s - loss: 0.3059 - categorical_accuracy: 0.8678\n",
      "Epoch 00050: val_categorical_accuracy did not improve\n",
      "1818/1818 [==============================] - 21s 12ms/step - loss: 0.3053 - categorical_accuracy: 0.8685 - val_loss: 0.5978 - val_categorical_accuracy: 0.7231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2769c08908>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = compute_class_weight('balanced'\n",
    "                                               ,[0,1,2,3,4,5]\n",
    "                                               ,labeled.label.apply(int).tolist())\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, \n",
    "                    input_length=max_length,\n",
    "                    #input_shape = (None,max_length),\n",
    "                    embeddings_regularizer = regularizers.l1(1e-5)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Conv1D(filters=700, kernel_size=2, padding='same', activation='tanh'))\n",
    "#model.add(Conv1D(filters=7, kernel_size=2, padding='same', activation='tanh'))\n",
    "#model.add(Conv1D(filters=50, kernel_size=3, padding='same', activation='sigmoid'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(Flatten())\n",
    "model.add(Dropout(0.7))\n",
    "model.add(LSTM(25, activation = 'tanh', kernel_regularizer = regularizers.l1(1e-5)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_hinge', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5',\n",
    "                               verbose=1, save_best_only=True,\n",
    "                               monitor = 'val_categorical_accuracy')\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=100,\n",
    "          validation_data = [X_test,y_test], callbacks=[checkpointer],\n",
    "         class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           Other       0.00      0.00      0.00        10\n",
      "           Crash       0.00      0.00      0.00        10\n",
      "Balance problems       0.45      1.00      0.62        10\n",
      " Synchronization       0.00      0.00      0.00        10\n",
      "        Positive       0.25      0.40      0.31        10\n",
      "             Bug       0.27      0.60      0.37        10\n",
      "\n",
      "     avg / total       0.16      0.33      0.22        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsm/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,roc_auc_score,accuracy_score\n",
    "def eval_classifier(input_text,model = model):\n",
    "    cleaned_text = clean_comment(input_text)\n",
    "    encoded_doc = t.texts_to_sequences([cleaned_text])\n",
    "    padded_doc = sequence.pad_sequences(encoded_doc, maxlen=max_length)\n",
    "    feats = padded_doc#vect.transform(padded_doc)\n",
    "    class_names = ['Other',\n",
    "        'Crash',\n",
    "        'Balance problems',\n",
    "        'Synchronization',\n",
    "        'Positive',\n",
    "        'Bug']\n",
    "    prediction = model.predict(feats)\n",
    "    #print(class_names[prediction[0]])\n",
    "    return np.argmax(prediction[0])\n",
    "def val_score(model):\n",
    "    class_names = ['Other',\n",
    "        'Crash',\n",
    "        'Balance problems',\n",
    "        'Synchronization',\n",
    "        'Positive',\n",
    "        'Bug']    \n",
    "    val_en = pd.read_excel('validation_en.xlsx')\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(0,6):\n",
    "        y_true.append([i]*10)\n",
    "        y_pred.append(val_en.iloc[:,i].apply(eval_classifier))\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    print(classification_report(y_true, y_pred, target_names = class_names))\n",
    "    return y_true,y_pred\n",
    "y_true,y_pred = val_score(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    4\n",
       "2    2\n",
       "3    2\n",
       "4    2\n",
       "5    2\n",
       "6    2\n",
       "7    4\n",
       "8    2\n",
       "9    4\n",
       "Name: Bug, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_en = pd.read_excel('validation_en.xlsx')\n",
    "val_en.Bug.apply(eval_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
